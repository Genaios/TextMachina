# Config for everything related to dataset generation inputs
input_config:
    # Dataset metadata
    domain: news
    language: en

    # Dataset generator parameters
    quantity: 10
    random_sample_human: true

    # HuggingFace dataset params
    dataset: xsum
    dataset_text_column: document
    dataset_params:
        split: test

    # Prompt template
    # For mixcase tasks, TextMachina's allows to use two strategies:

    # 1) Gap-based: Sample human sentences/word-spans boundaries
    # (one left-side and one right-side with a gap in between),
    # generate a random number of sentences/words in between
    # the boundary and then interleave the generations with 
    # the human boundaries. In this case, you have to prepare a
    # prompt template to generate sentences/words between a boundary
    # of two sentences/word-spans.

    # 2) Mask-based: Mask human sentences/word-spans and let the
    # LLM to reconstruct the masks. This approach has the advantage
    # of sending all the context to the LLM to fill the masks, instead
    # of relying only on a boundary of one preceding and one succeeding
    # sentence/word-span. However, this task requires to generate a
    # JSON output and generate all the masks, which is not addressable
    # by most of the LLMs. In this case, you have to prepare a prompt
    # template to generate masks given a text. Look the examples in
    # etc/examples/learning/extractors to figure out how to prompt
    # LLMs to generate datasets using the mask-based strategy.

    # The following prompt template illustrates the gap-based approach.
    template: >-
        Write {n} sentences to fill the gap marked as "____" between the following 2 sentences.
        Refrain to copy the sentences provided:\n
        {boundaries}\n\n
        Sentences:

    # Extractor params
    # For mixcase tasks, you can use `sentence_gap`, `sentence_masking`
    # `word_gap` or `word_masking` extractors.
    # Read the extractors' code within the extractors folder to
    # figure the arguments you can use for these extractors.
    extractor: sentence_gap
    max_input_tokens: 512
    extractor_args:
        sentence_gap:
            gap_token: "____"
            max_percentage_boundaries: 0.4
            max_sentence_span: 2

# Config for model instantiation
model_config:
    provider: openai
    model_name: gpt-3.5-turbo
    api_type: CHAT
    threads: 8
    max_retries: 5
    timeout: 120
    
# Decoding args
generation_config:
    # Ignore use `max_tokens` to get automatic length estimation
    max_tokens: 512
    temperature: 0.7
    presence_penalty: 1.0