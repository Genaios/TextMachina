# Config for everything related to dataset generation inputs
input_config:
    # Dataset metadata
    domain: news
    language: en

    # Dataset generator parameters
    quantity: 10
    random_sample_human: true

    # HuggingFace dataset params
    dataset: xsum
    dataset_text_column: document
    dataset_params:
        split: test

    # Prompt template
    # This prompt template example shows how to generate datasets using
    # non-instructed LLMs (note there is no instruction in the template).
    # Nothing prevents you to add an instruction here to work with
    # instructed LLMs.
    template: >-
        {sentences}. 

    # Extractor params
    # The `sentence_prefix` extractor extracts sentences from the
    # `dataset_text_column` column of each sample in a dataset and
    # fills the `{sentences}` placeholder of the prompt template.
    # The `word_prefix` extractor performs analogously to this one,
    # but the placeholder `{words}` must be used, instead of
    # `{sentences}` as in this case.
    extractor: sentence_prefix
    max_input_tokens: 256
    extractor_args:
        sentence_prefix:
            k: 1
    
    
# Config for model instantiation
# Requires `OPENAI_API_KEY=<key>` environment variable.
model_config:
    provider: openai
    model_name: gpt-3.5-turbo-instruct
    api_type: COMPLETION
    threads: 8
    max_retries: 5
    timeout: 120
    
# Decoding args
generation_config:
    # Ignore use `max_tokens` to get automatic length estimation
    # max_tokens: 100
    temperature: 0.7
    presence_penalty: 1.0