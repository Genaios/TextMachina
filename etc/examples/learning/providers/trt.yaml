# Config for everything related to dataset generation inputs
input_config:
    # Dataset metadata
    domain: tweets
    language: en

    # Dataset generator parameters
    quantity: 10
    random_sample_human: true

    # HuggingFace dataset params
    dataset: tweet_eval
    dataset_text_column: text
    dataset_params:
        split: train
        name: irony
    
    # Prompt template
    template: >-
        Write a tweet response to the following tweet:\n'{text}'\n\nTweet: 
    
    # Extractor params
    extractor: auxiliary
    max_input_tokens: 256
    
# Config for model instantiation
model_config:
    provider: inference_server
    base_url: http://localhost:8000/v2/models/ensemble/generate
    model_name: meta-llama/Llama-2-7b-hf
    inference_server: trt
    max_retries: 5
    threads: 8

# Decoding args
# For some inference servers, float parameters should be passed as strings.
generation_config:
    # Ignore `max_tokens` to get automatic length estimation
    max_tokens: 50
    temperature: 0.6